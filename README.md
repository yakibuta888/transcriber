# transcriber - 音声文字起こし・話者分離ツール

## 📝 プロジェクト概要

議事録作成業務の効率化を目的とした、**音声文字起こし**と**話者分離**を組み合わせた社内ツールです。
アセスメント業務を行う企業向けに開発し、非エンジニアでも直感的に操作・保守できるよう設計されています。

### ✨ 主な特徴

- **高精度な文字起こし**: Whisper-large、Kotoba-whisperなど複数のモデルから選択可能
- **話者分離機能**: 発言者を自動識別し、時系列で整理して表示
- **直感的なUI**: PySimpleGUIによる分かりやすいインターフェース
- **高速処理**: 非同期処理とGPU/Flash Attention最適化
- **運用性重視**: 詳細なログ出力とデバッグ機能

## 🚀 主な機能

### 🎯 コア機能
- 音声ファイルの自動文字起こし
- 話者の自動分離・識別
- 発言内容の時系列表示
- 複数の音声認識モデル対応

### ⚙️ 運用機能
- リアルタイム進捗表示
- 詳細なログ出力機能
- モデル選択・オプション設定UI
- ローカルモデル管理（外部依存の削減）

## 🛠️ 技術スタック

### フロントエンド
- **PySimpleGUI** - 直感的なデスクトップUI

### バックエンド・AI
- **Python 3.x**
- **Whisper-large** - 高精度音声認識
- **Kotoba-whisper** - 日本語特化音声認識
- **Speaker Diarization** - 話者分離

### アーキテクチャ・設計
- **ドメイン駆動開発（DDD）** - 責務の明確化
- **SOLID原則** - 保守性・拡張性の確保
- **非同期処理** - パフォーマンス最適化

## 🏗️ アーキテクチャ設計

### 設計思想
本プロジェクトは**保守性**と**拡張性**を重視し、以下の設計原則を採用しています：

#### 🎯 ドメイン駆動開発（DDD）
- ビジネスロジックをドメイン層に集約
- 責務を明確に分離し、変更に強い設計

#### 🔧 SOLID原則
- **単一責任原則**: 各クラスは単一の責任を持つ
- **開放閉鎖原則**: 拡張に開かれ、修正に閉じられた設計
- **依存関係逆転原則**: 抽象に依存し、具象に依存しない

#### 👥 非エンジニア対応
- 直感的なUI設計
- 分かりやすいログ出力
- 設定ファイルによる柔軟なカスタマイズ

## 📁 ファイル構成

```
src/
├── application/
│   └── services/
│       └── transcribe_service.py    # アプリケーションサービス層
├── domain/
│   ├── common/
│   │   ├── get_models_dir.py        # モデル管理
│   │   ├── output.py                # 出力処理
│   │   └── progress_reporter.py     # 進捗レポート
│   ├── exception/                   # 独自例外定義
│   ├── logics/
│   │   ├── audio_loader.py          # 音声読み込み
│   │   ├── kotoba_whisper_v2-2.py   # Kotoba-whisper実装
│   │   ├── merger.py                # 結果マージ処理
│   │   ├── speaker_diarizer.py      # 話者分離
│   │   └── whisper_large.py         # Whisper-large実装
│   └── services/
│       └── large_service.py         # ドメインサービス
├── log/
│   └── log_config.yaml              # ログ設定
├── models/                          # ローカルモデル保存
├── config.py                        # 設定管理
├── main.py                          # エントリーポイント
└── settings.py                      # アプリケーション設定
```

## 🚀 セットアップ・使用方法

### 必要な環境
- Python 3.8以上
- CUDA対応GPU（推奨）
- 8GB以上のメモリ

### 🤖 Hugging Faceトークンが必要な理由
- **モデルダウンロードの認証**: Hugging Face Hubからモデルをダウンロードする際、特に`whisper-large`や`kotoba-whisper`などの一部モデルではAPIトークンによる認証が必須です。
- **レート制限回避**: トークンなしではダウンロード回数制限（無料アカウントで1日10回）が適用され、業務利用に支障が出る可能性があります。
- **プライベートモデル対応**: 自社でファインチューニングしたモデルを利用する場合、トークンがアクセス権限として機能します。

### 🔑 Hugging Faceトークン取得手順
1. [Hugging Face公式サイト](https://huggingface.co/)でアカウント作成
2. 右上メニュー → `Settings` → `Access Tokens` 選択
3. `New token` ボタンでトークン生成（権限は`read`で十分）
4. 生成されたトークンをコピー

### ⚙️ トークン設定方法
**方法1: .envファイルを使用（推奨）**
```env
# .envファイルに追記
HUGGING_FACE_TOKEN="your_token_here"
```
- 起動時に自動読み込みされ、UI操作不要

**方法2: UI詳細オプションで設定**
- アプリ起動 → `詳細オプション`ボタン → `Hugging Faceトークン`欄に直接入力
- 毎回入力が必要だが、.envファイル編集が困難な環境向け

### 📝 モデル利用時の「同意」について
Hugging Faceの多くのモデル（例：whisper、kotoba-whisper、diarization等）は、**トークン発行だけでなく、モデルごとに利用規約やライセンスへの「同意」操作が必要な場合があります**。特に「Gated（ゲート付き）」リポジトリの場合、モデルのダウンロード前に明示的な同意が求められます。

#### なぜ同意が必要なのか
- モデルごとにライセンスや利用条件が異なり、商用利用やデータ利用に制限がある場合があるためです。
- 一部のモデルは、個別の利用規約や倫理的ガイドラインへの同意が必須となっています。
- ユーザーの同意履歴はHugging Face側で管理され、未同意の状態ではAPIやプログラムからのダウンロードがブロックされます。

### ✅ モデル利用手順（同意操作を含む）
1. **Hugging Faceアカウント作成・ログイン**
2. **利用したいモデルのページにアクセス**
   - 例：`https://huggingface.co/openai/whisper-large-v3`
3. **「Agree and Access」ボタンをクリック**
   - 「モデル利用規約に同意」や「ライセンスに同意」などのボタンが表示されます。
   - 内容を確認し、同意ボタンを押してください。
4. **同意完了後、プログラムやAPIからモデルを利用可能に**
   - PythonやCLIツールでのダウンロード時にトークンを利用
   - モデルによっては同意後でないとダウンロードが開始されません

#### ⚠️ 注意事項
- モデルごとのライセンスや利用条件は必ず事前に確認してください。
- 利用規約やライセンスに同意しない場合、そのモデルはダウンロード・利用できません。
- ライセンス違反は法的リスクやアカウント停止の原因となります。

### 📦 ローカルモデル導入手順
```bash
# modelsディレクトリで実行
git clone https://huggingface.co/openai/whisper-large-v3
git clone https://huggingface.co/rinna/kotoba-whisper-v2.2
```
- 必要に応じて特定バージョンを指定（例：`git checkout v3.0`）

### 🌐 自動モデル取得機能
- ローカルにモデルがない場合、ネット接続環境では自動でHugging Faceからダウンロード
- 初回起動時のみ発生し、次回以降はローカルモデルを優先使用

#### ⚠️ 注意事項
- モデルはHugging Faceからダウンロード後、システムのキャッシュディレクトリに永続的に保存されます。
- プログラム終了時に自動で削除されるわけではありません。
- 一時ファイルとして運用したい場合は、キャッシュディレクトリの管理を独自に行う必要があります。
- これにより、毎回のダウンロードによるネットワーク負荷や処理遅延を防止しています。

### 🛡️ ローカルモデルのメリット
| 項目 | メリット |
|------|----------|
| **安定性** | 外部サービスのAPI変更・ダウンタイムの影響を受けない|
| **セキュリティ** | 機密音声データを外部サーバーに送信しない |
| **処理速度** | ネットワークレイテンシが発生せず高速処理 |
| **オフライン対応** | インターネット接続不要で利用可能 |
| **バージョン固定** | モデル更新による突然の精度変化を防止 |

> **設計思想**: ローカルモデルを優先しつつ、ネット接続環境でもシームレスに利用可能な「フォールバック機構」を実装。これにより運用柔軟性を確保しています。

## 🖥️ 音声文字起こしツール利用ガイド

### インストール
```bash
# リポジトリのクローン
git clone https://github.com/yakibuta888/transcriber.git
cd transcriber

# 依存関係のインストール
pip install -r requirements.txt

# モデルの初期セットアップ
python setup_models.py
```

#### モデルの初期セットアップ
プロジェクトで利用するAIモデルを事前にダウンロードするためのスクリプトです。
初回のみ、以下を実行してください：
```bash
python setup_models.py
```
- 必要なHugging Faceトークンを.envファイルまたは詳細オプションで指定してください。
- モデルごとに利用規約への同意が必要な場合は、事前にHugging Faceの該当モデルページで「同意」操作を行ってください。
- セットアップ後は、models/フォルダにローカルモデルが保存され、以降の実行時は高速・安定して処理できます。

### 使用方法
```bash
# アプリケーションの起動
python src/main.py
```

起動後、以下の画面が表示されます：

![メイン画面](https://via.placeholder.com

1. **モデル選択**  
   - Whisper-large-v3（高精度多言語対応）  
   - Kotoba-whisper-v2.2（日本語特化）から選択  

2. **入力ファイル指定**  
   - [ファイル選択]ボタンで音声ファイル（mp3/wav等）を選択  

3. **出力設定**  
   - ☑️ 出力先を入力ファイルと同じフォルダにする（デフォルト選択）  
   - チェック解除時は出力フォルダを個別指定  
   - 出力ファイル名を入力（拡張子不要）  

4. **詳細オプション設定**  
   - [詳細オプション]ボタンで特殊設定可能：  
     ```python
     {
         '句読点付与': True,  # kotoba-whisper専用
         '話者数': 自動推定,
         'Flash Attention 2': False,  # GPU環境で有効化推奨
         'Hugging Faceトークン': ''  # 必須設定項目
     }
     ```

5. **実行**  
   - [実行]ボタンで処理開始  
   - 進捗バーとログ画面でリアルタイム状況を確認  

### 🔧 重要な設定項目

#### Hugging Faceトークン設定
**必須手順**:
1. 詳細オプション画面を開く
2. `Hugging Faceトークン`欄に取得したトークンを入力
3. 設定後はアプリ終了まで保持されます

> **注意**: トークン未設定時、モデルダウンロードが失敗します  
> 恒久的に使用する場合は`.env`ファイルに`HUGGING_FACE_TOKEN="your_token"`を追加

### 🚦 処理中の表示
- **プログレスバー**: 全体の進捗を％表示
- **ステータス欄**: 現在の処理フェーズ（例: 音声分割中）
- **ログ画面**: 詳細な処理ログとエラーメッセージ

### ⚠️ エラー対処法
| エラー内容 | 対処法 |
|-----------|--------|
| `モデル選択が不正です` | モデル名をWhisper/Kotobaから選択 |
| `入力ファイルが見つかりません` | ファイルパスを再確認 |
| `音声認識に失敗しました` | 音声ファイル形式を確認（16kHz推奨） |
| `予期せぬエラー` | ログ内容を開発者に連絡 |

### 💡 運用のコツ
- **初回実行時**: モデル自動ダウンロード（数分～数十分）
- **GPU環境推奨**: 詳細オプションで`Flash Attention 2`を有効化
- **長時間処理**: ログ画面で`chunk_length`値を小さく調整
- **話者分離精度向上**: `num_speakers`に想定話者数を指定

> 非エンジニア向け設計のため、エラー発生時はログ内容をコピーして管理者に問い合わせ可能

![詳細オプション画面](https://via.placeholder.com/ション設定画面の例（Hugging Faceトークン入力欄あり）*

## 💡 開発のポイント

### 🎯 実務を意識した設計
- **機密情報の適切な処理**: 社内ツールとして実際に運用中
- **非エンジニアでの保守性**: Pythonに触れたことがある程度でも対応可能
- **拡張性の確保**: 新しいモデルや機能の追加が容易

### ⚡ パフォーマンス最適化
- 非同期処理による並行実行
- GPU/Flash Attention活用
- ローカルモデル管理による安定性確保

### 🔍 運用・デバッグ対応
- 詳細なログ出力機能
- リアルタイム進捗表示
- エラーハンドリングとログ記録

## 📈 今後の拡張予定

- [ ] バッチ処理機能の追加
- [ ] Web版の開発
- [ ] 多言語対応の強化
- [ ] クラウド連携機能

---

## 🔗 技術的な質問やご相談

このプロジェクトに関する技術的な質問や、類似システムの開発についてご相談いただけます。
転職活動やココナラでの案件対応時には、より詳細な技術資料もご提供可能です。

**開発者**: [あなたの名前]  
**連絡先**: [連絡先情報]

